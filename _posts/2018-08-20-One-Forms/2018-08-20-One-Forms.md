---
title: One Forms
Edit: 2018-10-11
category: Blog
tags: One-form Topology
keywords: one-form manifold Topology covector dual-space tangent-vector
description: My interpretation on one-forms
mathjax: true
---

$$
\newcommand{\inner}[2]{\left\langle{#1,#2}\right\rangle}
\newcommand{\form}{\tilde}
%\renewcommand{\vec}{\mathbf}
\newcommand{\bra}[1]{\left\langle{#1}\right\vert }
\newcommand{\ket}[1]{\left| {#1}\right\rangle}
\newcommand{\braket}[2]{\left\langle {#1} \; \middle|\;{#2} \right\rangle }
\newcommand{\mani}{\mathcal}
\newcommand{\field}{\mathscr}
\newcommand{\Tspace}[1]{T\! {#1}}
\newcommand{\d}{\mathrm{d}}
\newcommand{\R}{\mathbb R}
\newcommand{\D}[2]{\frac{\d {#1}}{\d {#2} }}
\newcommand{\Partial}[2]{\frac{\partial {#1} }{\partial {#2} }}
\newcommand{\uvec}{\hat}
\newcommand{\dfdas}{: =}
\newcommand{\Eqn}[1]{\text{(Eqn. }\ref{#1}\text{)}}
\newcommand{\dual}{\tilde}
\newcommand{\vard}{\mathfrak d}
\newcommand{\vare}{\mathfrak e}
\newcommand{\e}{\mathrm e}
\newcommand{\i}{\mathrm i}
\newcommand{\blue}{\color{blue}}
\newcommand{\red}{\color{red}}
\newcommand{\norm}[1]{\left\|{#1}\right\|}
\newcommand{\set}[1]{\left\lbrace{#1}\right\rbrace}
\notag
$$

One form is a concept useful in integration, the integrand is a one-form. To perform integration on Manifold, one-form is critical.

# Curves and Functions

The definitions of curves and functions are as follow. A curve on a manifold is a linear map from interval $[a,b]$ to a set of points. Functions are defined over curves on manifold, *not* over curves in $\R^n$. With the help of local coordinates, curves and functions each have coordinate representations.

<img src="https://raw.githubusercontent.com/yk-liu/yk-liu.github.io/master/_posts/2018-08-20-One-Forms/assets/curve-function.png" width="80%" />

# General Vectors and Covectors

## Vectors

Roughly, a **vector space** is a set of entities such that is closed under linear combinations. Both "arrow heads" and linear functions satisfy this definition and thus form a vector space. Due to this remarkable fact, there exists a subset of vectors called the **basis** in the space, such that any vector in the space can be represented as a linear combination of members of the basis. 

Formally the vector space is a set of four things, $(V,k,+,*) $  where $ V $ is the set of vectors, $K$ is the **field** of scalars involved in creating multiples of vectors, $+$ is the function involved in adding two vectors and $ * $ is the binary function involved in multiplying a vector by a scalar.

## Covectors

Now consider the set of linear functions defined on $V$ (vectors) that have values in $K$ (numbers), i.e. $f:V\rightarrow K$. Such linear functions on vector spaces are called **linear functionals**. They apparently form a vector space $V^*$, since: 

- $(f_1+f_2)(V) = f_1(V) + f_2(V)$ where the R.H.S. is addition of two scalars in the field $K$. 
- $(kf)(V) = kf(V)$, again where the R.H.S. is multiplication of two scalars in the field $K$.

The vector space $V^*$ of linear functionals over $V$ is said to be **dual** to the vector space. 

## Inner Product and Dot Product

The **inner product** $\inner{\;}{\;}$ is defined as

$$
\inner{\dual a}{\vec b}=\dual{\vec a}(\vec b)\in\R
$$

Remember that covector is a linear functional, and the term $\dual{\vec a}(\vec b)$ is indeed a number.

Note that the inner product is defined between a vector and a dual vector and not between two vectors (like dot product). 

The inner product look suspiciously like dot product. A natural insight is that the **dot product** of two vectors is a real number, and thus one can "identify" a covector as a vector, inner product as dot product. This identification is just an isomorphism in the next section.

## Connect Covectors and Vectors with Isomorphism

If  the vector space is finite dimensional, so is its dual space. In this case, these two linear spaces have the same dimension and are thus isomorphic (see [here](https://en.wikipedia.org/wiki/Vector_space#Linear_maps_and_matrices)).

This isomorphism is fairly simple: just swap the basis and nothing is changed. 

Here we construct the **covector space** of $V$ and map from vectors to covectors:

1. Suppose the basis for the vector space $V$ is denoted as $(\uvec x_1,\uvec x_2, \uvec x_3)$, where $\uvec x_i$ is a unit vector in the positive $x_i$ direction. Suppose a basis for the dual space $\dual V$ is denoted as $(\d x_1,\d x_2,\d x_3)$ (this suspiciously looking name is carefully chosen for later elaboration, now you can either see it as a derivative $\d$ or simply abbreviation of "dual"). 

2. From the definition, a basis of a dual space is itself a dual vector, which acts on a vector, gives a real number.

   Writing that down as $\d x_i(\vec v)\in \R$.

3. Due to linearity of the vectors and covectors, $\d x_i(\vec v)$ can be seen as act on basis of vector
   $$
   \d x_i(\vec v) =\d x_i (v_1\uvec x_1+v_2\uvec x_2+v_3\uvec x_3)= v_1\d x_i (\uvec x_1)+v_2\d x_i (\uvec x_2)+v_3\d x_i (\uvec x_3)\in \R
   $$

4. Define $\d x_i (\uvec x_j)\dfdas \delta_{i,j}$. Recall that $\d x_i (\uvec x_j)=\inner{\d x_i}{\uvec x_j}=\delta_{ij}$.

5. **[map between bases of vectors and covectors]** A covector $\dual {\vec v}$  of vector $\vec v$ can be written in components $\dual{\vec v}=\dual v^\mu\d x_\mu$, conversely, a vector $\vec{\dual v}$ of a covector $\form v$, $\vec{\dual v}=\dual v^\mu \uvec x_\mu$ by directly interchanging $\d x_\mu$ between $\uvec x_\mu$.

6. A covector $\dual {\vec v}(\vec v)=\dual v^i \d x_i (v^i\uvec x_i)=\dual v^i v_i=\vec{\dual v }\cdot\vec v\in \R$

So the isomorphism can be just $V\leftrightarrow V^*,\quad \uvec x_i \leftrightarrow \d x_i$

> ***Remark:***
>
> - For vector spaces with finite bases the dual spaces are not very exotic; they are essentially the same as the original spaces. **There are some infinite dimensional vector spaces that have dual spaces that are different in nature from the original space**.
> - If the original space is finite dimensional, vectors can be sent to covectors through a isomorphism. For example, Dirac-bras $ \bra{v} $ and -kets $ \ket{v} $ are dual vectors, the contravariant vector $x^\mu$ and covariant vectors $x_\mu$ are dual to each other. The isomorphisms are both just transpose.
> - The aforementioned isomorphism can be written as a definition of map 
>
> $$
> {\red\form v} \leftrightarrow \vec v : \vec v \cdot \vec x =\inner{ { \red\form v} }{\vec x} \in \R
> \label{1formvec}
> $$
>
> - A Euclidean vector space comes with a dot product $(x, y) → x·y$, which can be used to describe one-forms in terms of vector fields (or equivalently, to identify cotangent vectors and tangent vectors):  Specifically, for every one-form $\form \omega$ there is a unique vector field ${\scr F }: \R^n\rightarrow V$  such that $\form ω_x(\vec v) \dfdas \vec{F}(x) · \vec{v}$ for all $x\in \R^n, v \in V$. [[Tao](http://www.math.ucla.edu/~tao/preprints/forms.pdf)] 
>

## Example of Covectors

For future reference, the dual spaces to the spaces of vector fields over Euclidean space is called **differential forms**. For now we will stick to the name covector space. At each point in the space $X$ there is a vector, say $\vec v$. This is equivalent to functions of the form $f:X\rightarrow V=(V,\R,+,*)$; i.e., functions which map every point of the space $X$ into the vector space $V$.

> ***IMPORTANT EXAMPLE:***
>
> The gradient and total derivative are dual vectors to each other. That's why above I made such a weird choice of name for covector basis. You can check that they fit in the above 
>
> $$
> \begin{align}
> \d\, f &= \Partial{f}{x^\mu} \,\d x^\mu\notag\\
> \updownarrow\phantom{f}&\phantom{\,} \text{ dual }\phantom{\,=}\updownarrow \label{df-nablaf}\\
> \vec\nabla f &=\Partial{f}{x^\mu}\,\uvec x^\mu\notag
> \end{align}
> $$
>
> Moreover, the action of $\d f$ on a vector (*any vector can be seen as the gradient of a function*) gives the derivative along the direction of that vector, which is indeed a real number.
> 
> $$
> \begin{align}
> \d f(\vec v)&=\Partial{f}{x^\mu}\d x^\mu(\vec v)\notag\\
> &=\Partial{f}{x^\mu}\d x^\mu(v^\nu \uvec x_\nu)\notag\\
> &=\Partial{f}{x^\mu}v^\nu \d x^\mu(\uvec x_\nu)\notag\\
> &=\Partial{f}{x^\mu}v^\nu \delta^\mu_\nu\label{df-nablaf-vec-1}\\
> &=\Partial{f}{x^\mu}v^\mu \notag\\
> &=\vec \nabla f\cdot \vec v \label{df-nablaf-vec-2}\\
> &\dfdas\nabla_{\vec v}f=\lim_{h\rightarrow0}\frac{f(\vec x + h\vec v)-f(\vec x)}{h}\in\R \label{directionalderivative}
> \end{align}
> $$
>



# Vectors on Manifolds

We all know what vector is in Euclidean space, a **vector** is formally defined as an element of a **vector space**, i.e. a set that is closed under finite vector addition and scalar multiplication.

Though on a manifold, things are a little different. There are three equivalent ways of defining vectors. To put them vaguely,

1. Vector is (like in high school) an arrow and can be seen as a tuple of numbers;
2. Vector is an equivalent class of curves;
3. Vector is a (derivation) operator.

These definitions are equivalent to each other [(proof)](https://maths-people.anu.edu.au/~andrews/DG/DG_chap4.pdf). Here is my two cents.

## Vector is an arrow

When I think about vector on a manifold, I have the picture of some arrow tangent to the "surface" of manifold. 

Since in general there is no way to define a "straight arrow" connecting two points. Vectors can only be "tangent vectors". In other words, vectors cannot live on the manifold itself, but the collection of tangent spaces over the entire manifold, called the **tangent bundle**. That way, the vector is a kept at a geometrical view. This is a generalization of the notion of a bound vector in a Euclidean space. But this requires embedding the manifold in some higher dimensional space, which is not very convenient, since Differential Geometry aims at investigating the space (or maybe space-time) without jumping out of it.

<img src="https://raw.githubusercontent.com/yk-liu/yk-liu.github.io/master/_posts/2018-08-20-One-Forms/assets/drawing_tangent_vector.JPG" width="90%">

Obviously, once the vector is in an ambient space, it can be represented by a tuple of numbers. How to represent a vector although "it can be represented by a tuple of numbers"? There will be different basis on each point of a manifold and

## Vector is a (derivation) operator

The word "tangent" cries for derivation. Besides the naïve vector-as-arrow picture lacks necessary algebraic tools to perform calculations with.

Still, the problem is that vectors do not live on the manifold. This forbids defining "what is vector" using vocabularies from manifold, only allowing us to tell "what is in one-to-one correspondence with vector". So what is in one-to-one correspondence with vector? The answer could be a (derivation) operator. 

Let's start with a concrete example. Here is a curved surface being the manifold in focus embed in $\R^3$. Although this is not always possible for any manifold, we are going to use this as a tool to gain some intuition. 

<img src="https://raw.githubusercontent.com/yk-liu/yk-liu.github.io/master/_posts/2018-08-20-One-Forms/assets/drawing_tangent_vector_as_operator.png" width="90%">

First we are going to see the surface in $ \R^3​ $ as $\vec r(x,y,z) = Const.​$ On this surface we can have different local coordinates $(u,v)​$ (left) or $(w,s)​$ (right). With a curve $\Sigma​$ on the surface we can have both a using embedded coordinates as $\vec r(t)=(x(t),y(t),z(t))​$, or local coordinates as $\vec r(t) = (u(t),v(t))​$ or $\vec r(t) = (w(t),s(t))​$. The tangent vector $X​$ at $p=\vec r(t_0)=(u_0,v_0)=(w_0,s_0)​$ has a very simple definition in $\R^3 ​$ as $\vec X = \D{\vec r(t)}{t}​$. Using the chain rule, 

$$
\begin{align}
\vec X &= \D{\vec r(t)}{t} \notag \\
&=\left.\Partial{\vec r(u,v)}{u}\right\vert_{\substack{u=u_0\\ v=v_0}} \left.\D{u(t)}{t}\right\vert_{t=t_0}+ \left.\Partial{\vec r(u,v)}{v}\right\vert _{\substack{u=u_0\\v=v_0}}\left.\D{v(t)}{t}\right\vert_{t=t_0} \notag \\
&=\dot u(t_0)\ \left.\Partial{\vec r(u,v)}{u}\right\vert_{\substack{u=u_0\\v=v_0}} +\dot v(t_0)\ \left.\Partial{\vec r(u,v)}{v}\right\vert_{\substack{u=u_0\\v=v_0}}
\label{vectorToOperator}
\end{align}
$$

The expression is now of the form $a \Partial{\vec r}{u}+b \Partial{\vec r}{v}$. Meaning **[any vector have a "component" form using differential operators]**. 

We will see that the term $\Partial{\vec r}{u}$ is indeed a coordinate dependent basis vector. If we take the curve $\Gamma$ as $\displaystyle {\begin{cases} u =u(t)\\ v =2 \end{cases}}$ in the left figure (or $w=3$ in the right), $\Eqn{vectorToOperator}$ becomes

$$
\begin{align}
\vec X &= \D{\vec r(t)}{t} \notag \\
&=\left.\Partial{\vec r(u,v)}{u}\right\vert_{\substack{u=u_0\\v=v_0}} \left.\D{u(t)}{t}\right\vert_{t=t_0}+0 \notag\\
&=\dot u (t_0)\ \left.\Partial{\vec r(u,v)}{u}\right\vert_{\substack{u=u_0\\v=v_0}} 
\end{align}
$$

notice for example,  $ \displaystyle {\Gamma=\begin{cases}u=k\cdot t\\ v=2\end{cases} , \vec X =k\left.\Partial{\vec r}{u}\right\vert_{\substack{u=u_0\\ v=v_0}}} $ or $ \displaystyle {\Gamma=\begin{cases}u=\tan t\\ v=2\end{cases} , \vec X =\left.\tan(t_0)\Partial{\vec r}{u}\right\vert_{\substack{u=u_0\\ v=v_0}}} $  This means if we take a parametrized curve along v=2, the tangent vector at point p will always be some number times $ \left.\Partial{\vec r}{u}\right\vert_{\substack{u=u_0 \\ v=v_0}} $ .  In other words, **[the partial differential operator *IS* a unit vector]** .

Reiterating, a tangent vector is  ...




> [[Sean Carroll](https://arxiv.org/pdf/gr-qc/9712019.pdf)] Let’s imagine that we wanted to construct the tangent space at a point $p$ in a manifold
> $\mani M$, using only things that are intrinsic to $\mani M$ (no embeddings in higher-dimensional spaces etc.). One first guess might be to use our intuitive knowledge that there are objects called “tangent vectors to curves” which belong in the tangent space. In some coordinate system $x^\mu$ any curve through $p$ defines an element of $\R^n$ specified by the $n$ real numbers $\d x^\mu/\d \lambda $ (where $\lambda $ is the parameter along the curve), but this map is clearly coordinate-dependent, which is not what we want.
>
> Nevertheless we are on the right track, we just have to make things independent of coordinates. Then we notice that each smooth curve (represented by smooth function $f$) through $p$ defines an operator, namely the directional derivative, which maps $f → \d f/\d\lambda$ (at $p$).
>
> > The tangent space $T_p$ can be **identified** (since we cannot define it) with the space of directional derivative operators along curves through $p$. 
> >
>
> To establish this idea we must demonstrate three things:
>
> 1. **[Verify the space of directional derivatives is a vector space.]** That is to say, this operator is a vector in the sense that it behave nicely in addition and scalar product. For two operators $\displaystyle \frac{\d}{\d\lambda}$ and $\displaystyle \frac{\d}{\d\eta}$ representing derivatives along two curves through $p$, there is no problem adding these and scaling by real numbers, to obtain a new operator $a\displaystyle \frac{\d}{\d\lambda}+b\frac{\d}{\d\eta}$. The next step is to verify that the space closes; i.e., that the resulting operator is itself a derivative operator.
>
>    By definition, a derivative operator is one that acts linearly on functions, and obeys the conventional Leibniz (product) rule on products of functions. We have
>
>    $$
>    \begin{align*}
>    \left(a\frac{\d }{\d \lambda}+b\frac{\d }{\d \eta}\right)(fg)& =a f\frac{\d g}{\d \lambda}+ag\frac{\d f}{\d \lambda}+bf\frac{\d g}{\d \eta}+bg\frac{\d f}{\d \eta}\\
>    &=\left(a\frac{\d f}{\d\lambda}+b\frac{\d f}{\d\eta}\right)g+\left(a\frac{\d g}{\d\lambda}+b\frac{\d g}{\d\eta}\right)f
>    \end{align*}
>    $$
>    As we had hoped, the product rule is satisfied, and the set of directional derivatives is therefore a vector space.
>
> 2. **[Find a basis for the space.]** Consider again a coordinate chart with coordinates $x^\mu$. The partial derivative operators $\{\partial_\mu\}$ at $p$ form a basis for the tangent space $T_p$. (It follows immediately that $T_p$ is $n$-dimensional, since that is the number of basis vectors.) To see this we will show that any directional derivative can be decomposed into a sum of real numbers times partial derivatives. **This is in fact just the familiar expression for the components of a tangent vector**, but it’s nice to see it from the big-machinery approach. Consider an $n$-manifold $\mani M$, a coordinate chart $\varphi: \mani M \rightarrow \R^n$, a curve $c : [a,b] \rightarrow \mani M$, and a function $f : \mani M \rightarrow \R$. This leads to the following tangle of maps: 
>
>    <img src="https://raw.githubusercontent.com/yk-liu/yk-liu.github.io/master/_posts/2018-08-20-One-Forms/assets/vector.png" width="80%"/>
>
>    If $t$ is the parameter along $c$, we want to expand the vector/operator $\displaystyle \frac{\d}{\d t}$ in terms of the partials $\partial_{\mu}$. Using the chain rule, we have
>    $$
>    \begin{align*}
>    \frac{\d}{\d t}f&=\ \frac{\d}{\d t}(f\circ c)\\
>     &=\frac{\d}{\d t}[(f\circ\varphi^{-1})\circ(\varphi\circ c)] \\
>     &=\frac{\d(\varphi\circ c)^{\mu}}{\d t}\frac{\partial(f\circ\varphi^{-1})}{\partial x^{\mu}}\\
>     &= \frac{\d x^{\mu}}{\d t}\partial_{\mu}f 
>     \end{align*}
>    $$
>
>    The first line simply takes the informal expression on the left hand side and rewrites it as an honest derivative of the function $(f\circ \gamma)$ : $\R\rightarrow \R$. The second line just comes from the definition of the inverse map $\phi^{-1}$ (and associativity of the operation of composition). The third line is the formal chain rule, and the last line is a return to the informal notation of the start. Since the function $f$ was arbitrary, we have
>    $$
>    \begin{align}
>    \frac{\d}{\d t}=\frac{\d x^{\mu}}{\d t}\partial_{\mu} \label{vector-decomposition-to-partial}
>    \end{align}
>    $$
>    Thus, the partials $\{\partial_{\mu}\}$ do indeed represent a basis for the vector space of directional derivatives, which we can therefore safely identify with the tangent space.
>
> 3. **[Show the operator is some vector pointing along a certain direction.]** Of course,  the vector represented by $\displaystyle \frac{\d}{\d t}$ is one we already know; it's the tangent vector to the curve with parameter $t$. Thus $\Eqn{vector-decomposition-to-partial}$ can be thought of as a restatement of vector decomposition, where we claimed that the components of the tangent vector were simply $ \d x^{\mu}/\d t$. The only difference is that we are working on an arbitrary manifold, and we have specified our basis vectors to be $\uvec e_\mu=\partial_\mu$.
>
>    This particular basis $\uvec e _ \mu=\partial _ \mu $ is known as a coordinate basis for $T _p$; it is the formalization of the notion of setting up the basis vectors to point along the coordinate axes. There is no reason why we are limited to coordinate bases when we consider tangent vectors; it is sometimes more convenient, for example, to use orthonormal bases of some sort. However, the coordinate basis is very simple and natural, and we will use it almost exclusively throughout the course.
>




The directional derivative of z along the vector v is the differential operator v acting on z.

## Vector is an equivalent class of curves

This is a strange sentence. Here it goes from [Nakahara](http://stringworld.ru/files/Nakahara_M._Geometry_topology_and_physics_2nd_ed..pdf):

> To be more mathematical, we introduce an equivalence class of curves in $\mani M$. If two curves $c_1(t)$ and $c_2(t)$ satisfy
>
> 1. $c_1(0) = c_2(0) = p$
> 2. $ \left. \frac{\d x^\mu(c_1(t))}{\d t} \right\rvert_{t=0} =\left.\frac{\d x^\mu(c_2(t))}{\d t}\right\rvert_{t=0}$
>
> then $c_1(t)$ and $c_2(t)$ yield the same differential operator $X$ at $p$, in which case we deﬁne $c_1(t) ~ c_2(t)$. Clearly $ ~ $ is an equivalence relation and deﬁnes the equivalence classes. We identify the tangent vector X with the equivalence class of curves
> $[c(t)] = \left\lbrace \tilde{c}(t) \mid \tilde{c}(0)=c(0) \text{ and } \left.\frac{\d x^\mu (\tilde{c}(t))}{\d t} \right\vert_{t=0} = \left.\frac{\d x^\mu (c(t))}{\d t} \right\vert_{t=0} \right\rbrace $
>
> rather than a curve itself.











in **vector** is defined to be a **tangent vector** to a curve in $\mani M$. Note the existence of a vector is independent of the coordinate or function. Like shown below, the "Big red arrow" $X$ tangent to the manifold depend solely on the curve and the point, not any local coordinate. The directional derivative of a curve in $\R^n$ is easily defined as

## Vector is derivation







a vector should really be interpreted as derivatives (in a sense derivations are vectors), then an "arrowhead" pointing somewhere. The counterpart of vector is covector, which is called one-form, acting like the integration. The intuitive "arrowhead" interpretation will be mentioned. 

you can see [here(why vector fields are represented as differential operators on manifolds?)](https://math.stackexchange.com/questions/1027742/vector-fields-on-manifolds): it is because, in general, one cannot define a vector on a manifold as an arrow between two points on the manifold (as it generally will not be Euclidean) and as such the best we can do is consider the tangent space at each point on the manifold. Now, we could define vectors as the directional derivatives of the coordinate curves at that point, however this is very much coordinate dependent, which doesn't fit in with the notion of a vector. As such, if we consider tangent vector at a point as a differential operator that defines a tangent space at that point, then we satisfy the axioms of a vector space and thus achieve the notion of vectors on manifolds?



**vector** is defined to be a **tangent vector** to a curve in $\mani M$. Note the existence of a vector is independent of the coordinate or function. Like shown below, the "Big red arrow" $X$ tangent to the manifold depend solely on the curve and the point, not any local coordinate.

<img src="https://raw.githubusercontent.com/yk-liu/yk-liu.github.io/master/_posts/2018-08-20-One-Forms/assets/vector.png" width="80%"/>

The directional derivative of a curve in $\R^n$ is easily defined as

$$
\begin{align}
X(0)&=\left.\D{f(c(t))}{t}\right\vert_{t=0}\notag\\
&=\left.\D{((f \circ \varphi^{-1})(\varphi\circ c )(t))}{t}\right\vert_{t=0}\notag\\
&=\left.\D{(f \circ \varphi^{-1})(\vec x )}{t}\right\vert_{t=0}\notag\\
&=\left.\D{((f \circ \varphi^{-1})(x_1(t),x_2(t),\cdots))}{t}\right\vert_{t=0}\notag\\
&=\left.\Partial{(f \circ \varphi^{-1})}{x^\mu}\D{x^\mu(t)}{t}\right\vert_{t=0}\notag\\
&=\left.\Partial{(\_ \circ \varphi^{-1})}{x^\mu}\D{x^\mu(t)}{t}\right\vert_{t=0} f\notag\\
&=\D{x^\mu(t)}{t} \left.\Partial{(\_ \circ \varphi^{-1})}{x^\mu}\right\vert_{t=0} f\notag\\
&\dfdas  X^\mu \left.\Partial{(\_ \circ \varphi^{-1})}{x^\mu}[f]\right\vert_{t=0} \label{vec-component}\\
&\dfdas \left.X[f]\right\vert_{t=0} \label{vec-def}
\end{align}
$$

> ***Remark:***
>
> 1. A tangent vector is not $X[f]$, but $X$. Tangent vector is independent of $\varphi$ or $f$.
> 2. Evidently, due to linearity of derivatives, a tangent vector is still a linear map $X(\alpha f+\beta g) = \alpha X(f) + \beta X(g) $.
> 3. Tangent vectors live on a completely different space, i.e. tangent space. It is *not* the limit of two point on the smooth manifold moving near to each other along a curve, which is still on the manifold. 

Now a vector is a operator act on a function, defined by a curve $c$ and a function $f$ in terms of the directional derivative along the curve. 

Tangent vectors have decompositions $\Eqn{vec-component}$. The $\{\uvec{e}^\mu\} \dfdas \Partial{(\_ \circ \varphi^{-1})}{x^\mu} $ is taken as **coordinate basis**, $X^\mu$ are naturally called the **components** of $X$ with respect to $\uvec{e}^\mu$. 

Reverse the order of definition, a vector can be defined as mathematical objects with the form $X={v_p}^\mu\Partial{}{x^\mu}$, or simply:

> A tangent vector at a point of a manifold is a linear map that satisfies the Leibniz rule.


## Vector field

A vector field can be seen as arrows sprinkled on a manifold as shown in the left. Or it defines a map from a point to a curve on manifold as shown in the right. 

>  A point follows the direction of vector field moves at a "velocity" of the magnitude of the vector, tracing out a curve on the manifold.

<img src="https://raw.githubusercontent.com/yk-liu/yk-liu.github.io/master/_posts/2018-08-20-One-Forms/assets/vector-curve.png" width="80%"/>

#  One-forms

## Covector Revisited

A **one-form**, or a **covector**, **dual vector**, is an element of a **dual vector space**.

A dual vector space is all linear functions that maps a vector to $\R^1$. In another word, 

> A dual vector act on a vector gives a real number.
>
> A dual vector is a linear, real-valued function of vectors.

## One Forms on Manifold

According to $\Eqn{directionalderivative}$ of [Example of Covectors](#example-of-covectors), the one-form of a tangent vector is the derivative of the function along the vector.

But the tangent vector is on a manifold is in some sense a "directional derivative", and the dual of "directional derivative" is a "gradient", and *vice versa*. 

Recall that a vector is defined as $X\dfdas X^\mu\uvec x_\mu=\D{x^\mu}{t} \Partial{}{x^\mu}$, and vector 's action on a function $f$ is defined as $X[f]\dfdas \D{x^\mu}{t} \Partial{f}{x^\mu}$. 

The total derivative $\d $ denoted as a vector as $\vard$. Being a vector, $\vard$ can have it's component form of $\vard = \vard^\mu \uvec d_\mu$, with $\uvec{d}_\mu$ an operator, $\vard^\mu$ a real number seen as a coefficient of linear combination. Write down the component form of this map,

$$
\begin{align}
\inner{\vard}{X}&=\vard (\D{x^\mu}{t} \Partial{}{x^\mu})\notag\\
&=\vard_\nu\uvec{d}^\nu (\D{x^\mu}{t} \Partial{}{x^\mu})\notag\\
\xrightarrow{\text{linearity}}&=\left(\vard_\nu \D{x^\mu}{t}\right)\uvec{d}^\nu ( \Partial{}{x^\mu})\notag\\
\small\text{notice that $\Partial{}{x^\mu}$ is }&\small \text{simply a basis like in $\Eqn{df-nablaf-vec-1}$}\notag\\
&=\left(\vard_\nu \D{x^\mu}{t}\right)\uvec{d}^\nu (\uvec x_\mu)\notag\\
&=\left(\vard_\nu \D{x^\mu}{t}\right)\delta^\nu_\mu \\
&=\vard_\mu \D{x^\mu}{t}\label{dxDef}
\end{align}
$$

The basis of total derivative can now be expressed as $\uvec d^\mu(\uvec x^\nu)=\uvec{d}^\mu ( \Partial{}{x^\nu})=\delta^{\mu,\nu}$. 

Mapping the one-form (total derivative) to a covector (gradient), and take the inner product of the vector and the covector gives $\d x^\mu \cdot \Partial{}{x^\mu}=1$. 

> ***Remark:***
>
> Despite the fact that the covector (gradient) is a vector, it's obvious that gradient lives in a different vector space as tangent vectors does, since their basis is completely different objects. Moreover, There are no dot product defined in tangent space without further mapping, because the product of two tangent vector bases $\Partial{}{x^\mu}$ would not be a number.

Now it's clear that a tangent vector is defined as $X\dfdas X^\mu\uvec x_\mu=\D{x^\mu}{t} \Partial{}{x^\mu}$, the corresponding one-forms are defined as $\dual X \dfdas  X^\mu \dual x_\mu=\D{x^\mu}{t} \d x_\mu$. 

## Example of one-form and Vector

<img src="https://raw.githubusercontent.com/yk-liu/yk-liu.github.io/master/_posts/2018-08-20-One-Forms/assets/vector.png" width="80%"/>

The vector is the directional derivative of a curve. The one-form is actually the total derivative of the function $f$. The covector is the gradient of the field $f$ over a open set.



## Redefined Vector and one-forms

At the end of [Vectors on a Manifold](#Vectors-on-a-Manifold), vectors were generalized as mathematical objects with the form $X=X^\mu\Partial{}{x^\mu}$. Similarly, the corresponding one-form can be generalized as $\form X= X^\mu \d x_\mu$. This definition will immediate cause a problem: it's no longer guaranteed that a one-form is a total derivative of some function. $x\d y$ is a perfect one-form by this definition, but it is no function's total derivative.

# Two-Forms and Higher Forms

## Wedge Product of General Vectors

The cross product of vectors $\vec u \times \vec v$ is a very useful operation in $3$ dimensional geometry. It determines the area of the **parallelogram** containing these vectors and the plane containing it. A **wedge product** is the analogue used to determine a high dimensional parallelograms.

The wedge (楔) product (楔积) $\wedge$ is a special kind of tensor product. 

$$
\vec x^{\mu_1} \wedge \vec x^{\mu_2} \wedge. . .\wedge \vec x^{\mu_r} = \sum_{P\in \mathbb S_r} \operatorname{sgn}(P) \vec x^{\mu_{P(1)}} \otimes\vec x^{\mu_{P(2)}}\otimes \cdots\otimes \vec x^{\mu _{P(r)}} \label{wedgeDef}
$$

For example, 

$$
\begin{align}
\vec u \wedge \vec v &= \vec u \otimes \vec v - \vec v \otimes \vec u\\
\vec u \wedge \vec v  \wedge \vec w &= \vec u \otimes \vec v  \otimes \vec w + \vec w \otimes \vec u \otimes \vec v  + \vec v  \otimes \vec w \otimes \vec u \label{wedgeExample} \\
&- \vec u \otimes \vec w \otimes \vec v  - \vec w \otimes \vec v  \otimes \vec u - \vec v \otimes \vec u \otimes \vec w \notag
\end{align}
$$

Given a vector space $V$, a space of wedge product can be constructed as

$$
\wedge ^2 V = \set{ \vec u\wedge\vec v  {\mid}  \vec u,\vec v \in V  }\\
\wedge ^3 V = \set{ \vec u\wedge\vec v \wedge \vec w {\mid} \vec u,\vec v,\vec w \in V  }\\
\wedge ^n V = \set{ \vec u_1\wedge\vec u_2 \wedge\cdots\wedge \vec u_n {\mid} \vec u_i\in V ,i=1,2,\cdots,n }
$$

Considering wedge products actually being $n\times n$ antisymmetric tensors can be seen as vectors, it's no wonder that this space is indeed a vector space.

[[John](https://www.av8n.com/physics/area-volume.pdf)] There is a **norm** for a wedge product (seen as a bi-vector, tri-vector, or $n$-vector) defined as 
$$
\begin{align}
\norm{A \wedge B}^2&\dfdas(A \wedge B)\cdot(B \wedge A)\notag\\
&=-(A \wedge B)^2
\end{align}
$$


> ***CONNECTIONS TO GEOMETRIC ENTITIES***:
>
> 1. **Analogue to cross product as a test of collinearity**: The wedge product gives a simple way to test for "**coplanarity**" or linear (in)dependence of vectors: if $\vec u$ and $\vec v$ are collinear, meaning $\vec u = a \vec v$, by anti-symmetry of wedge product,  
>     $$
>     \vec u \wedge \vec v =\vec u \wedge a\vec u=a(\vec u \otimes \vec u - \vec u \otimes \vec u)=0 \notag
>     $$
>
>     If $\vec w$ is coplanar with $\vec u$ and $\vec v$, meaning $\vec w = a \vec u + b \vec v$, (“collapsed box”, not maximally linear independent), then
>
>     $$
>     \vec w \wedge \vec u \wedge \vec v = a\vec u  \wedge \vec u \wedge \vec v+b\vec v\wedge \vec u \wedge \vec v = 0 \notag
>     $$
>
> 2. **Analogue to cross product as a indicator of orientation**: If $n\gt 3$, there are infinitely many directions perpendicular to the two vectors, so you can't think of the orientation as a vector (like the cross product in three dimensions). Instead, you may think of the orientation as a *circle* in the plane of the two given vectors $\vec u$ and $\vec v$, with a direction attached to it in one of the two possible ways: $\circlearrowleft$ or $\circlearrowright$.
>
> 3. **Analogue to cross product as a way to compute "area of parallelogram"**: For two vectors $\vec u=(a,b,c)$ and $\vec v=(d,e,f)$, We can see that the nonzero entries of wedge product are basically the same as for the cross product. 
>
>     $$
>     \begin{align}
>     \vec{u} \wedge \vec{v}
>     &=(u_1,u_2,u_3)\wedge(v_1,v_2,v_3)\notag\\
>     &= \begin{pmatrix}
>      0        & \red u_1v_2 − u_2v_1  & \red u_1v_3 − u_3v_1\\
>      \blue −u_1v_2 + u_2v_1 & 0        & \red u_2v_3 − u_3v_2\\
>      \blue −u_1v_3 + u_3v_1 & \blue −u_2v_3 + u_3v_2 & 0        
>     \end{pmatrix}\notag\\
>     &= (u_1 v_2 - u_2 v_1) (\uvec{e}_1 \wedge \uvec{e}_2) + (u_3 v_1 - u_1 v_3) (\uvec{e}_3 \wedge \uvec{e}_1) + (u_2 v_3 - u_3 v_2) (\uvec{e}_2 \wedge \uvec{e}_3) \label{wedgetensorvector}\\
>          \notag\\
>     \vec{u} \times \vec{v}
>     &=(u_1, u_2, u_3) \times (v_1, v_2, v_3) \notag\\
>     &= {\red(u_2v_3 − u_3v_2)}\uvec i + {\red(−u_1v_3 + u_3v_1)}\uvec j + {\red(u_1v_2 − u_2v_1)}\uvec k\notag\\
>          \end{align}
>     $$
>
>     > **Note:**
>     >
>     > - The wedge product is a tensor, not just a matrix. It's better to flatten the matrix as a (column or row) vector with 9 bases with 3 redundant bases and 3 null bases as shown in $\Eqn{wedgetensorvector}$.
>     >
>     > - This matrix is anti-symmetry matrix of odd dimension and thus has a zero determinant.
>     >
>
>     However, this result is not the area of this two vectors. $\vec u \wedge\vec v$ is a bivector, it's norm $A^2=\norm{\vec u\wedge \vec v}^2\overset{\small\text{numerically}}{=\!=\!=\!=\!=}(\vec u \times \vec v)^2$ is the area of the parallelogram.
>
> 4. **Generalization as a direction way to calculate $n$-dimensional area, (specially, $3$-dimensional area being the volume)**: the $n$-dimensional area is defined as a $n$ wedge product of $n$-dimensional vectors. For $n=3$,$ \vec{u} \wedge \vec{v} \wedge \vec{w} = (u_1 v_2 w_3 + u_2 v_3 w_1 + u_3 v_1 w_2 - u_1 v_3 w_2 - u_2 v_1 w_3 - u_3 v_2 w_1) (\uvec{e}_1 \wedge \uvec{e}_2 \wedge \uvec{e}_3) $. Still the volume($3$-dimensional area) $V^2=\norm{\vec{u} \wedge \vec{v} \wedge \vec{w} }$.
>
>     There is more to it. While $\vec{u} \wedge \vec{v} \wedge \vec{w}$ is a simple construction of three vectors, it is also a wedge product of vector and yet a wedge product $\vec{u} \wedge (\vec{v} \wedge \vec{w})$. The volume of the parallelepiped ($3$-dimensional area) is now the span of a vector and an parallelogram ($2$-dimensional area). Similarly, a $(n+m)$-dimensional area can be spanned by a $n$-dimensional area and $m$-dimensional area.
>

## Wedge Product of One-Forms

Naturally, like in [[Redefined Vector and one-forms](#Redefined-Vector-and-one-forms)] a 2-form is of the form $X=X_{\mu\nu}\d x^\mu\d x^\nu$, e.g., $\e^x\d x\d y+2x^2\d y \d z+ (y-x)\d x\d z$. 

This can be seen as a tensor product of two one-forms.
$$
\begin{align}
(3\d x + \d y) ∧ (\e^x\d x + 2\d y) &= 3\e^x\d x ∧ \d x + 6\d x ∧ \d y + \e^x \d y ∧ \d x + 2\d y ∧ \d y\\
&= (6 − \e^x)\d x ∧ \d y
\end{align}
$$

## Exterior Derivative

Another way to see a two-form is that it's a second derivative of something. So take the "derivative" of a $1$-form (first derivative) should result in a $2$-form. This procedure is called a **exterior derivative**. This operation is denoted using again the symbol $\d$.

$$
\begin{align*}
\d (\form X) &= \d (X^\mu \d x_\mu)\\
&=(\d X^\mu )\wedge\d x_\mu + X^\mu\d( \d x_\mu)\\
\xrightarrow[\d( \d x_\mu)=0]{\text{by def.}}&=(\d X^\mu )\wedge\d x_\mu\\
\xrightarrow[\Eqn{dxDef}]{\text{See}}&=\inner{\vard}{X^\mu}\wedge\d x_\mu\\
{\tiny\text{see $\Eqn{df-nablaf-vec-1}$}}&=\Partial{X^\mu}{x_\nu}\d x_\nu \wedge\d x_\mu \\
\xrightarrow[\Eqn{wedgeExample}]{\text{See}}&=\Partial{X^\mu}{x_\nu}\d x_\nu \wedge\d x_\mu 
\end{align*}
$$

For example, 

$$
\d(F \d x+G\d y +H\d z) = (\Partial{G}{x} −\Partial{F}{y})\d x\wedge \d y + (\Partial{H}{y} −\Partial{G}{z})\d y \wedge \d z + (\Partial{F}{z} − \Partial{H}{x})\d z \wedge \d x
$$

This coincides with the definition of **curl**.
$$
\vec{\nabla} \times \vec X = (Hy − Gz)\uvec i + (Gx − Fy)\uvec k + (Fz − Hx)\uvec j
$$




Thus the exterior derivative of a $p$-form is a $(p+1)$-form.


A $2$-form is an expression built using wedge products of pairs of $1$-forms. 









The real significance of $2$-forms will come later when we do surface integrals. A $2$-form will be an expression that can be integrated over a surface in the same way that a $1$-form can be integrated over a curve.
